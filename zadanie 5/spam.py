# -*- coding: utf-8 -*-
"""spam.ipynb
https://www.kaggle.com/uciml/sms-spam-collection-dataset
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MICRGNKmFsBrJgurxsVqp4felHdz7eMN
"""

# Commented out IPython magic to ensure Python compatibility.
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.style.use('seaborn-deep')
from sklearn.metrics import confusion_matrix
import nltk
import os
import string
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer 
import nltk

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

nltk.download('stopwords')
df = pd.read_csv('/content/spam.csv')
df.head()
# Any results you write to the current directory are saved as output.

df.info()

df.groupby('Category').describe()

df['Length'] = df['Message'].apply(len)
df.head()

explode = (0.1,0)  
fig1, ax1 = plt.subplots(figsize=(12,7))
ax1.pie(df['Category'].value_counts(), explode=explode,labels=['ham','spam'], autopct='%1.1f%%',
        shadow=True)
# Equal aspect ratio ensures that pie is drawn as a circle
ax1.axis('equal')  
plt.tight_layout()
plt.legend()
plt.show()

plt.figure(figsize=(10,6))
df['Length'].plot.hist(bins = 150)

df['Length'].describe()

df[df['Length'] == 910]['Message'].iloc[0]

def text_process(mess):
    """
    Takes in a string of text, then performs the following:
    1. Remove all punctuation
    2. Remove all stopwords
    3. Returns a list of the cleaned text
    """
    # Check characters to see if they are in punctuation
    nopunc = [char for char in mess if char not in string.punctuation]

    # Join the characters again to form the string.
    nopunc = ''.join(nopunc)
    
    # Now just remove any stopwords
    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]

df['Message'].head(10).apply(text_process)

bow_transformer = CountVectorizer(analyzer=text_process).fit(df['Message'])

print(len(bow_transformer.vocabulary_))

message4 = df['Message'][3]
print(message4)

bow4 = bow_transformer.transform([message4])
print(bow4)
print(bow4.shape)

print(bow_transformer.get_feature_names()[4066])
print(bow_transformer.get_feature_names()[9551])

messages_bow = bow_transformer.transform(df['Message'])

print('Shape of Sparse Matrix: ', messages_bow.shape)
print('Amount of Non-Zero occurences: ', messages_bow.nnz)

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer().fit(messages_bow)
tfidf4 = tfidf_transformer.transform(messages_bow)
print(tfidf4)

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=10, criterion='entropy',random_state=0)
classifier.fit(tfidf4, df['Category'])

print('predicted:', classifier.predict(tfidf4)[0])
print('expected:', df.Category[3])

all_predictions = classifier.predict(messages_bow)
print(all_predictions)

from sklearn.metrics import classification_report
print (classification_report(df['Category'], all_predictions))

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(df['Category'], all_predictions))

from sklearn.model_selection import train_test_split

msg_train, msg_test, label_train, label_test = \
train_test_split(df['Message'], df['Category'], test_size=0.2)

print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))

from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts
    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
    ('classifier', RandomForestClassifier()),  # train on TF-IDF vectors w/ SVM
])

pipeline.fit(msg_train,label_train)

predictions = pipeline.predict(msg_test)

from sklearn.metrics import confusion_matrix,classification_report
cm = confusion_matrix(label_test,predictions)
class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cm), annot=True, cmap="BuPu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print(classification_report(predictions,label_test))

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(predictions,label_test))